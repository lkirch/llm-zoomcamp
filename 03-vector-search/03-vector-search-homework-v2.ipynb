{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d7c19d-67f2-4875-97bb-80819de8cd26",
   "metadata": {},
   "source": [
    "## Homework: Vector Search (Version 2)\n",
    "\n",
    "In this homework, we'll experiment with vector search with and without Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef39aa8-5600-44b6-a2ab-89d427427dc9",
   "metadata": {},
   "source": [
    "### Q1. Getting the embeddings model\n",
    "\n",
    "First we will get the embeddings model multi-qa-distilbert-cos-v1 from the [Sentence Transformer library](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)\n",
    "\n",
    "```\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "```\n",
    "\n",
    "Create the embedding for this user question:\n",
    "\n",
    "```\n",
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "```\n",
    "\n",
    "What is the first value of the resulting vector?\n",
    "\n",
    "* -0.24\n",
    "* -0.04\n",
    "* 0.07\n",
    "* 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df247496-1dea-467b-83bf-8160138c021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e94544d-2221-4f49-bbb7-ada97496a291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.078222655"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "\n",
    "embeddings = embedding_model.encode(user_question)\n",
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62532d9c-7591-47b4-b5bc-65ca7020a446",
   "metadata": {},
   "source": [
    "### A1. 0.07 is closest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07da5e-d043-46c3-8f20-0d2f09b3f3ae",
   "metadata": {},
   "source": [
    "For question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eda8afc-a147-4eab-a6a0-fec4cfd8faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765527f9-183c-45d6-a407-3edf36efa716",
   "metadata": {},
   "source": [
    "### Prepare the documents\n",
    "\n",
    "Now we will create the embeddings for the documents.\n",
    "\n",
    "Load the documents with ids that we prepared in the module:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08089296-9e2d-49d0-896a-574d847fde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f443ed8e-0901-42f3-88b9-db72adad6b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21a29f-c1db-48ab-ab0f-21893aa2b648",
   "metadata": {},
   "source": [
    "We will use only a subset of the questions - the questions for \"machine-learning-zoomcamp\".  After filtering, you should have only 375 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c7b3d7-f38c-4461-aa8b-ff2ba9b7c28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': 'c02e79ef'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f60b764-fd8a-4e0b-9bab-e0f3080514f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs = [d for d in documents if d['course'] == 'machine-learning-zoomcamp']\n",
    "len(filtered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f88dd4-85f4-452f-a1d6-b6291c921e20",
   "metadata": {},
   "source": [
    "### Q2. Creating the embeddings\n",
    "\n",
    "Now for each document, we will create an embedding for both the question and answer fields.\n",
    "\n",
    "We want to put all of them into a single matrix X:\n",
    "\n",
    "* Create a list of embeddings\n",
    "* Iterate over each document\n",
    "* qa_text = f'{question} {text}'\n",
    "* compute the embedding for qa_text, append it to embeddings\n",
    "* at the end, let X = np.array(embeddings) (import numpy as np)\n",
    "\n",
    "What is the shape of X? ( X.shape ). Include the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52de1eb4-72cb-4d11-b4d4-e4f7f2f5b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "# creating a dense vector using the pre-trained model\n",
    "embeddings = []\n",
    "for doc in filtered_docs:\n",
    "    question = doc['question']\n",
    "    text = doc['text']\n",
    "    qa_text = f'{question} {text}'\n",
    "    embeddings.append(model.encode(qa_text).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68955ab0-6844-441f-8838-c17c1a6211c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(embeddings)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b405f-87c0-4d1d-b4b4-09103e0bd8b3",
   "metadata": {},
   "source": [
    "### A2. The shape of X is (375,768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343af651-b6da-4c71-9c70-8cb52ee123e4",
   "metadata": {},
   "source": [
    "### Q3. Search\n",
    "\n",
    "We have the embeddings and the query vector.  Now let's compute the cosine similarity between the vector from Q1 (let's call it v) and the matrix from Q2.\n",
    "\n",
    "\n",
    "The vectors returned from the embedding model are already normalized (you can check it by computing a dot product of a vector with itself - it should return 1.0).  This means that in order to compute the cosine similarity, it's sufficient to multiply the matrix X by the vectort v:\n",
    "\n",
    "```\n",
    "scores = X.dot(v)\n",
    "```\n",
    "\n",
    "What's the highest score in the results?\n",
    "\n",
    "* 65.0\n",
    "* 6.5\n",
    "* 0.65\n",
    "* 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad567202-7a0e-46b8-97db-4f4c6749d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = X.dot(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f7bfd9-7742-4154-a2b1-21fd9c9c48a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6506573240979582"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b56541-02ed-4a18-8aec-abd35f44833e",
   "metadata": {},
   "source": [
    "### A3. The maximum score is 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c1ba2-3f4c-4f29-85dc-3b1b1983d1b8",
   "metadata": {},
   "source": [
    "### Vector search\n",
    "\n",
    "We can now compute the similarity between a query vector and all the embeddings.\n",
    "\n",
    "Let's use this to implement our own vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028d7ddf-f287-494b-b915-28fa7d750f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Homework - What are homework and project deadlines?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a1daf537'},\n",
       " {'text': 'After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Homework and Leaderboard - what is the system for points in the course management platform?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '29865466'},\n",
       " {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2ed9b986'},\n",
       " {'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Could not start docker image “control-center” from the docker-compose.yaml file.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '70ac8e80'},\n",
       " {'text': 'You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\nYou might face some challenges, especially for Windows users. If you face cnd2\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\nUsing GitHub Codespaces\\nSetting up the environment on a cloudV Mcodespace\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ddf6c1b3'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorSearchEngine():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query, num_results=10):\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        idx = np.argsort(-scores)[:num_results]\n",
    "        return [self.documents[i] for i in idx]\n",
    "\n",
    "search_engine = VectorSearchEngine(documents=documents, embeddings=X)\n",
    "search_engine.search(v, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573f43c-3f15-42e2-ae84-3cbac2bb4adc",
   "metadata": {},
   "source": [
    "If you don't understand how the search function works:\n",
    "\n",
    "* Ask ChatGTP or any other LLM of your choice to explain the code\n",
    "* Check our pre-course workshop about implementing a search engine here\n",
    "\n",
    "(Note: you can replace argsort with argpartition to make it a lot faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e72bd-f100-4a60-9da9-d6a29869a731",
   "metadata": {},
   "source": [
    "### ___Will need to complete later___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63213d94-0906-4ce4-b871-de3777edc478",
   "metadata": {},
   "source": [
    "### Q4. Hit-rate for our search engine\n",
    "\n",
    "Let's evaluate the performance of our own search engine. We will use the hit rate metric for evaluation.\n",
    "\n",
    "Hit Rate (HR) or Recall at k:\n",
    "```\n",
    "    Measures the proportion of queries for which at least one relevant document is retrieved in the top k results.\n",
    "    Formula: HR@k = (Number of queries with at least one relevant document in top k) / |Q|\n",
    "```\n",
    "\n",
    "First, load the ground truth dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94db6117-b028-49ed-a149-5626964f032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e80b2b-ac77-4251-a7b1-16bc4aef613a",
   "metadata": {},
   "source": [
    "Now use the code from the module to calculate the hit rate of VectorSearchEngine with num_results=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a8e796-5582-4f4f-8c9b-3e9e37781108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Where can I sign up for the course?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'document': '0227b872'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a8a215c-0053-4c76-a64a-f41e58cf08f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1830"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc2f3d70-4c3e-4d6c-bba5-9a6b3cf9c9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19fb4277492418988de10375478e7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = search_engine.search(v, num_results=5)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41fb5c93-8c04-4bfd-89d1-f0ceddfd839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84312a06-f37e-4f4c-bfb3-e928ff641038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03da9c-41f6-4d64-86ba-b2c289ea7a47",
   "metadata": {},
   "source": [
    "What did you get?\n",
    "\n",
    "* 0.93\n",
    "* 0.73\n",
    "* 0.53\n",
    "* 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c92e9-52ef-4e8d-9b57-db8bd4ce771b",
   "metadata": {},
   "source": [
    "### A4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beefaea-62aa-41e4-96f9-cdd3ca0eef8a",
   "metadata": {},
   "source": [
    "### Q5. Indexing with Elasticsearch\n",
    "\n",
    "Now let's index these documents with elasticsearch\n",
    "\n",
    "```\n",
    "    Create the index with the same settings as in the module (but change the dimensions)\n",
    "    Index the embeddings (note: you've already computed them)\n",
    "```\n",
    "After indexing, let's perform the search of the same query from Q1.\n",
    "\n",
    "What's the ID of the document with the highest score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1e7b0-5514-4737-a3fe-19aeb8830edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e82d6-a76c-4c13-a95f-4c47161eb0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a247ec1-33a2-4796-9340-600245c500c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42d75f-e7a1-43cc-84c3-de8ad69e18f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b289c2f6-762c-4bc6-a65f-dc1110ead821",
   "metadata": {},
   "source": [
    "### Q6. Hit-rate for Elasticsearch\n",
    "\n",
    "The search engine we used in Q4 computed the similarity between the query and ALL the vectors in our database. Usually this is not practical, as we may have a lot of data.\n",
    "\n",
    "Elasticsearch uses approximate techniques to make it faster.\n",
    "\n",
    "Let's evaluate how worse the results are when we switch from exact search (as in Q4) to approximate search with Elastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a3240-d5f5-44a8-be9f-21975de2489c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "367fddad-0549-4496-b743-f33e3685d426",
   "metadata": {},
   "source": [
    "What's hit rate for our dataset for Elastic?\n",
    "\n",
    "* 0.93\n",
    "* 0.73\n",
    "* 0.53\n",
    "* 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a08bb4-c8ef-4792-9714-a3439fbdf340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
