{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d7c19d-67f2-4875-97bb-80819de8cd26",
   "metadata": {},
   "source": [
    "## Homework: Vector Search (Version 2)\n",
    "\n",
    "In this homework, we'll experiment with vector search with and without Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef39aa8-5600-44b6-a2ab-89d427427dc9",
   "metadata": {},
   "source": [
    "### Q1. Getting the embeddings model\n",
    "\n",
    "First we will get the embeddings model multi-qa-distilbert-cos-v1 from the [Sentence Transformer library](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)\n",
    "\n",
    "```\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "```\n",
    "\n",
    "Create the embedding for this user question:\n",
    "\n",
    "```\n",
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "```\n",
    "\n",
    "What is the first value of the resulting vector?\n",
    "\n",
    "* -0.24\n",
    "* -0.04\n",
    "* 0.07\n",
    "* 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df247496-1dea-467b-83bf-8160138c021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e94544d-2221-4f49-bbb7-ada97496a291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.078222655"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "\n",
    "embeddings = embedding_model.encode(user_question)\n",
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62532d9c-7591-47b4-b5bc-65ca7020a446",
   "metadata": {},
   "source": [
    "### A1. 0.07 is closest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765527f9-183c-45d6-a407-3edf36efa716",
   "metadata": {},
   "source": [
    "### Prepare the documents\n",
    "\n",
    "Now we will create the embeddings for the documents.\n",
    "\n",
    "Load the documents with ids that we prepared in the module:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08089296-9e2d-49d0-896a-574d847fde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21a29f-c1db-48ab-ab0f-21893aa2b648",
   "metadata": {},
   "source": [
    "We will use only a subset of the questions - the questions for \"machine-learning-zoomcamp\".  After filtering, you should have only 375 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f88dd4-85f4-452f-a1d6-b6291c921e20",
   "metadata": {},
   "source": [
    "### Q2. Creating the embeddings\n",
    "\n",
    "Now for each document, we will create an embedding for both the question and answer fields.\n",
    "\n",
    "We want to put all of them into a single matrix X:\n",
    "\n",
    "* Create a list of embeddings\n",
    "* Iterate over each document\n",
    "* qa_text = f'{question} {text}'\n",
    "* compute the embedding for qa_text, append it to embeddings\n",
    "* at the end, let X = np.array(embeddings) (import numpy as np)\n",
    "\n",
    "What is the shape of X? ( X.shape ). Include the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de1eb4-72cb-4d11-b4d4-e4f7f2f5b3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "343af651-b6da-4c71-9c70-8cb52ee123e4",
   "metadata": {},
   "source": [
    "### Q3. Search\n",
    "\n",
    "We have the embeddings and the query vector.  Now let's compute the cosine similarity between the vector from Q1 (let's call it v) and the matrix from Q2.\n",
    "\n",
    "\n",
    "The vectors returned from the embedding model are already normalized (you can check it by computing a dot product of a vector with itself - it should return 1.0).  This means that in order to compute the cosine similarity, it's sufficient to multiply the matrix X by the vectort v:\n",
    "\n",
    "```\n",
    "scores - X.dot(v)\n",
    "```\n",
    "\n",
    "What's the highest score in the results?\n",
    "\n",
    "* 65.0\n",
    "* 6.5\n",
    "* 0.65\n",
    "* 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad567202-7a0e-46b8-97db-4f4c6749d58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483c924-f192-43f6-a36b-48f683e79bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
