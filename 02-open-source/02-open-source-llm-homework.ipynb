{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624f9eea-054f-47db-b28b-b7abd7a0dba7",
   "metadata": {},
   "source": [
    "### Homework: Open-Source LLMs\n",
    "___\n",
    "In this homework, we will experiment more with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10986ae2-6caf-4adf-937e-72b6541dacd0",
   "metadata": {},
   "source": [
    "### Q1. Running Ollama with Docker\n",
    "___\n",
    "Let's run ollama with Docker. We will need to execute the same command as in the lectures:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fad6f94f-bebe-413d-a1f6-b135578b27c8",
   "metadata": {},
   "source": [
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450ce8e-5795-44d5-a5ec-b61c023acf09",
   "metadata": {},
   "source": [
    "What's the version of ollama client?\n",
    "\n",
    "To find out, enter the container and execute ollama with the -v flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16234630-a8be-4b95-9abc-bbfd2f836b8f",
   "metadata": {},
   "source": [
    "0.1.47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8c9d9-83af-431f-b02e-8b479ca920a7",
   "metadata": {},
   "source": [
    "### Q2. Downloading an LLM\n",
    "____\n",
    "\n",
    "We will download a smaller LLM - gemma:2b.\n",
    "\n",
    "Again let's enter the container and pull the model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ade5da5-4d5b-4fbc-8189-375d2216aea5",
   "metadata": {},
   "source": [
    "ollama pull gemma:2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdddca-5507-4e6f-b114-d1384511d8d7",
   "metadata": {},
   "source": [
    "In docker, it saved the results into /root/.ollama\n",
    "\n",
    "We're interested in the metadata about this model. You can find it in models/manifests/registry.ollama.ai/library\n",
    "\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3ddc9-d751-4b76-81af-c0fe1f799e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64b83d6-ecb0-4a64-afd8-1ef22ed50cad",
   "metadata": {},
   "source": [
    "### Q3. Downloading the weights\n",
    "----\n",
    "We don't want to pull the weights every time we run a docker container.  Let's do it once and have them available every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the /root/.ollama folder to a named volume, let's map it to a local directory:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fbbc71f-0aab-4d72-8407-41acb56b8c2c",
   "metadata": {},
   "source": [
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd5be3-0d9f-4374-8e0a-6b19c4a6d315",
   "metadata": {},
   "source": [
    "Now pull the model:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe9af948-d302-4538-bd11-ab9a4ffbf67b",
   "metadata": {},
   "source": [
    "docker exec -it ollama ollama pull gemma:2b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa70470-d9a8-4549-9967-7bbc0a690c52",
   "metadata": {},
   "source": [
    "What's the size of the ollama_files/models folder?\n",
    "\n",
    "* 0.6G\n",
    "* 1.2G\n",
    "* 1.7G\n",
    "* 2.2G\n",
    "\n",
    "Hint: on linux, you can use du -h for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978ef9b-9db6-409b-a88b-737b9c21636c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0684f07-1860-47cc-a70f-cd61f7080da5",
   "metadata": {},
   "source": [
    "### Q5. Adding the weights\n",
    "---\n",
    "Let's now stop the container and add the weights to a new image\n",
    "\n",
    "For that, let's create a Dockerfile:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "100ed8c4-a09b-4bc2-b3b4-7c3e46f73045",
   "metadata": {},
   "source": [
    "FROM ollama/ollama\n",
    "\n",
    "COPY ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f436059-938d-4afc-800b-e42adc7160e7",
   "metadata": {},
   "source": [
    "What do you put after COPY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1df507-ef67-4e97-9a7c-a7d453904feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3cfdc1-74af-4d2b-b8cd-d5fef23122b5",
   "metadata": {},
   "source": [
    "### Q6. Serving it\n",
    "---\n",
    "Let's build it:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f18acd6-38b9-4cdc-825e-a06b9bcc244a",
   "metadata": {},
   "source": [
    "docker build -t ollama-gemma2b ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572eb94-d66b-4292-a4a1-40f469c88cce",
   "metadata": {},
   "source": [
    "And run it:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3916bcfd-e8cf-47f8-a204-447825d5bbd4",
   "metadata": {},
   "source": [
    "docker run -it --rm -p 11434:11434 ollama-gemma2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92677f5e-c43f-4063-9d29-054f3034d323",
   "metadata": {},
   "source": [
    "We can connect to it using the OpenAI client\n",
    "\n",
    "Let's test it with the following prompt:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23eba662-57d1-4a3b-a7ad-27510bd86ceb",
   "metadata": {},
   "source": [
    "prompt = \"What's the formula for energy?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7320f66-7b3a-48d3-bb8c-e5cf228f812c",
   "metadata": {},
   "source": [
    "Also, to make results reproducible, set the temperature parameter to 0:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c31f61f-9577-4d9a-90ce-1e5bb4b54fa3",
   "metadata": {},
   "source": [
    "response = client.chat.completions.create(\n",
    "    #...\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32545580-3edb-447c-8ea6-2f8277edaf52",
   "metadata": {},
   "source": [
    "How many completion tokens did you get in response?\n",
    "\n",
    "* 304\n",
    "* 604\n",
    "* 904\n",
    "* 1204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7325382f-db73-43cb-a686-e18d07a0918f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
